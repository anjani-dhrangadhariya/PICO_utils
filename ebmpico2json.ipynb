{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232ab10c",
   "metadata": {},
   "source": [
    "# Parse EBM-PICO to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48f03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2426d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_docs = '/mnt/nas2/data/systematicReview/ebm_nlp_2_00/documents'\n",
    "indir_annots = '/mnt/nas2/data/systematicReview/ebm_nlp_2_00/annotations/aggregated/starting_spans'\n",
    "outdir = '/mnt/nas2/data/systematicReview/PICO_datasets/EBM_parsed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61e4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = glob.glob(f'{indir_docs}/*.tokens')\n",
    "pos_tags = glob.glob(f'{indir_docs}/*.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be10d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_pico = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3659e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents\n",
    "for i, doc in enumerate(tokens):\n",
    "    \n",
    "    inner_dict = dict()\n",
    "    \n",
    "    k = doc.split('/')[-1]\n",
    "    k_i = k.split('.')[0]\n",
    "    lines = list( open(doc, 'r') )\n",
    "    v = [ w.strip() for w in lines ]\n",
    "    ebm_pico[ k_i ] = {} \n",
    "    ebm_pico[ k_i ]['tokens'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3155343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read document POS tags\n",
    "for i, doc in enumerate(pos_tags):\n",
    "\n",
    "    inner_dict = dict()\n",
    "\n",
    "    k = doc.split('/')[-1]\n",
    "    k_i = k.split('.')[0]\n",
    "    lines = list( open(doc, 'r') )\n",
    "    v = [ w.strip() for w in lines ]\n",
    "    #ebm_pico[ k_i ] = {}\n",
    "    if k_i in ebm_pico:\n",
    "        ebm_pico[ k_i ]['pos'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac594a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read document labels\n",
    "for entity in ['participants', 'interventions', 'outcomes']:\n",
    "    \n",
    "    entity_labels = glob.glob(f'{indir_annots}/{entity}/test/gold/*.*')\n",
    "    for i, doc in enumerate(entity_labels):\n",
    "        k = doc.split('/')[-1]\n",
    "        k_i = k.split('.AGGREGATED')[0]\n",
    "        \n",
    "        lines = list( open(doc, 'r') )\n",
    "        v = list(map(lambda s: s.strip(), lines)) \n",
    "        if k_i in ebm_pico:\n",
    "            ebm_pico[ k_i ][entity] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e49c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_pico_keys = list( ebm_pico.keys() )\n",
    "for k in ebm_pico_keys:\n",
    "    if k in ebm_pico:\n",
    "        if len(ebm_pico[k]) < 3:\n",
    "            del ebm_pico[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4c92a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to the JSON file\n",
    "with open(f'{outdir}/test_ebm.json', 'w+') as fp:\n",
    "    json.dump(ebm_pico, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5587d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing JSON for Hilfiker set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ba8c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_physio = '/mnt/nas2/results/Results/systematicReview/systematicReviews/data/TA_screening/hilfiker_sr_ta/PICO_annotation_project/validation_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2c5a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_physio = glob.glob(f'{indir_physio}/tokens/*.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c8e7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "physio = dict()\n",
    "\n",
    "# Read documents\n",
    "for i, doc in enumerate(tokens_physio):\n",
    "    \n",
    "    inner_dict = dict()\n",
    "    \n",
    "    k = doc.split('/')[-1]\n",
    "    k_i = k.split('.')[0]\n",
    "    lines = list( open(doc, 'r') )\n",
    "    v = [ w.strip() for w in lines ]\n",
    "    physio[ k_i ] = {} \n",
    "    physio[ k_i ]['tokens'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2a529a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "indir_physio_pos = '/mnt/nas2/results/Results/systematicReview/systematicReviews/data/TA_screening/EBM_NLP/allSentence_annot/hilfiker_sentence_annotation2POS.txt'\n",
    "\n",
    "with open(indir_physio_pos, 'r') as f:\n",
    "    \n",
    "    for i in f:\n",
    "        data = json.loads(i)\n",
    "        all_sents = []\n",
    "        for k,v in data.items():\n",
    "            key = k.split('.')[0]\n",
    "            for k_sent, v_sent in v.items():\n",
    "                all_sents.extend( v_sent[2] )\n",
    "        if key in physio:\n",
    "            assert len( physio[key]['tokens'] ) == len(all_sents)\n",
    "            physio[key]['pos'] = all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a0c3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read document labels\n",
    "for entity in ['participants', 'interventions', 'outcomes']:\n",
    "    entity_labels = glob.glob(f'/mnt/nas2/results/Results/systematicReview/systematicReviews/data/TA_screening/hilfiker_sr_ta/PICO_annotation_project/validation_files/labels/{entity}/annot/*.AGGREGATED.ann')\n",
    "    for i, doc in enumerate(entity_labels):\n",
    "        k = doc.split('/')[-1]\n",
    "        k_i = k.split('.AGGREGATED')[0]\n",
    "\n",
    "        lines = list( open(doc, 'r') )\n",
    "        v = list(map(lambda s: s.strip(), lines)) \n",
    "        if k_i in physio:\n",
    "            if len(v) != len( physio[ k_i ]['pos'] ):\n",
    "                #print( len(v) , len( physio[ k_i ]['pos'] ) , len( physio[ k_i ]['tokens'] ) )\n",
    "                v = v[ 0 : len( physio[ k_i ]['pos'] ) ]\n",
    "            assert len(v) == len( physio[ k_i ]['pos'] ) == len( physio[ k_i ]['tokens'] )\n",
    "            physio[ k_i ][entity] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a82e5e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TA_21731', 'TA_23001', 'TA_3772', 'TA_17907', 'TA_22078', 'TA_18584', 'TA_11654', 'TA_1490', 'TA_20229', 'TA_2890', 'TA_17877', 'TA_16952', 'TA_25536', 'TA_12782', 'TA_20301', 'TA_22114', 'TA_18350', 'TA_25390', 'TA_10975', 'TA_10997', 'TA_17723', 'TA_3507', 'TA_25050', 'TA_1285', 'TA_10873', 'TA_23149', 'TA_10858', 'TA_14139', 'TA_3532', 'TA_12731', 'TA_10900', 'TA_22048', 'TA_2556', 'TA_25329', 'TA_19295', 'TA_25277', 'TA_2886', 'TA_10558', 'TA_15902', 'TA_13456', 'TA_1514', 'TA_13093', 'TA_16545', 'TA_2681', 'TA_11800', 'TA_19310', 'TA_21989', 'TA_10066', 'TA_22635', 'TA_1334', 'TA_22607', 'TA_21604', 'TA_17690', 'TA_3258', 'TA_13245', 'TA_15895', 'TA_3492', 'TA_18574', 'TA_25339', 'TA_24830', 'TA_20706', 'TA_25652', 'TA_16977', 'TA_10470', 'TA_1920', 'TA_3393', 'TA_17701', 'TA_17709', 'TA_23325', 'TA_18607', 'TA_16812', 'TA_1860', 'TA_15560', 'TA_18439', 'TA_17176', 'TA_20156', 'TA_16979', 'TA_15382', 'TA_1367', 'TA_19655', 'TA_13393', 'TA_14868', 'TA_1589', 'TA_1012', 'TA_3391', 'TA_24737', 'TA_3335', 'TA_19293', 'TA_23666', 'TA_13472', 'TA_2586', 'TA_17372', 'TA_16978', 'TA_12413', 'TA_15652', 'TA_11840', 'TA_17483', 'TA_17910', 'TA_17348', 'TA_23830', 'TA_3430', 'TA_24778', 'TA_1651', 'TA_13282', 'TA_17737', 'TA_15038', 'TA_14560', 'TA_17973', 'TA_1662', 'TA_20468', 'TA_18739', 'TA_11688', 'TA_2830', 'TA_12813', 'TA_2921', 'TA_11140', 'TA_11130', 'TA_13540', 'TA_14229', 'TA_3648', 'TA_17840', 'TA_23409', 'TA_17837', 'TA_11132', 'TA_1168', 'TA_23300', 'TA_14233', 'TA_20020', 'TA_21903', 'TA_22224', 'TA_10945', 'TA_24682', 'TA_24907', 'TA_15558', 'TA_20365', 'TA_23394', 'TA_2007', 'TA_1967', 'TA_22124', 'TA_13169', 'TA_2093', 'TA_20796', 'TA_11071', 'TA_13862', 'TA_2314', 'TA_2831', 'TA_20082', 'TA_11798', 'TA_1679', 'TA_12923', 'TA_19155', 'TA_21612', 'TA_13012'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physio.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7a0c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to the JSON file\n",
    "with open(f'{outdir}/test_physio.json', 'w+') as fp:\n",
    "    json.dump(physio, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547a690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
