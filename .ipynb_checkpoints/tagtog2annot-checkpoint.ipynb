{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c8b756",
   "metadata": {},
   "source": [
    "# tagtog to PICO annotation parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d3e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "\n",
    "# Connect to tagtog API\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from requests.auth import HTTPBasicAuth, HTTPDigestAuth\n",
    "import urllib\n",
    "\n",
    "# Specific imports\n",
    "import numpy\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a431ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response for query is:  <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Get all the entities\n",
    "all_annotations_data = 'https://www.tagtog.com/-api/metrics/v0/search_stats?project=PICO_test_ebm&owner=anjDhr&search=*'\n",
    "#all_annotations_data_response = get(all_annotations_data, auth=('anjDhr', '9J@NiScMhUy9LbR'))\n",
    "all_annotations_data_response = get(all_annotations_data, auth=('anjDhr', '9J@NiScMhUy9LbR'))\n",
    "print('The response for query is: ', all_annotations_data_response)\n",
    "all_annotations_data_response = json.loads(all_annotations_data_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3dd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2name = dict()\n",
    "\n",
    "for eachEntry in all_annotations_data_response:\n",
    "    if 'e_' in eachEntry:\n",
    "        ent2name[eachEntry] = all_annotations_data_response[eachEntry]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a2eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e_8': 'Intervention_name',\n",
       " 'e_16': 'Outcome_name',\n",
       " 'e_3': 'Participant_age',\n",
       " 'e_2': 'Participant_symptom',\n",
       " 'e_18': 'Outcomes_other',\n",
       " 'e_7': 'StudyType',\n",
       " 'e_5': 'Participant_samplesize',\n",
       " 'e_11': 'Intervention_device',\n",
       " 'e_19': 'Outcome_AE',\n",
       " 'e_15': 'Intervention_comparator',\n",
       " 'e_6': 'Participant_other',\n",
       " 'e_17': 'Outcome_measure',\n",
       " 'e_10': 'Intervention_components',\n",
       " 'e_4': 'Participant_gender',\n",
       " 'e_1': 'Participant_disease'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8da56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_ents = {}\n",
    "I_ents = {}\n",
    "O_ents = {}\n",
    "S_ents = {}\n",
    "for k, v in ent2name.items():\n",
    "    if str(v).startswith('Intervention'):\n",
    "        I_ents[k] = v\n",
    "    if str(v).startswith('Participant'):\n",
    "        P_ents[k] = v\n",
    "    if str(v).startswith('StudyType'):\n",
    "        S_ents[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41f35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tk = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8eb897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjani/pico/lib/python3.6/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize pos tagger\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe8b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 My       PRON   PRP$   poss   pronoun              pronoun, possessive\n",
      "       3 name     NOUN   NN     nsubj  noun                 noun, singular or mass\n",
      "       8 is       AUX    VBZ    ROOT   auxiliary            verb, 3rd person singular present\n",
      "      11 a        DET    DT     det    determiner           determiner\n",
      "      13 random   ADJ    JJ     attr   adjective            adjective (English), other noun-modifier (Chinese)\n"
     ]
    }
   ],
   "source": [
    "string_our = 'My name is a random'\n",
    "doc = nlp('My name is a random')\n",
    "temp_offset = []\n",
    "for token in doc:\n",
    "    print(f'{token.idx:{8}} {token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')\n",
    "    temp_offset.append( token.idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a0cb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 8, 11, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a54862f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_our[temp_offset[0]:temp_offset[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0172a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_dir = '/mnt/nas2/data/systematicReview/PICO_datasets/annotation_me/PICO_test_ebm_pis/ann.json/master/pool'\n",
    "plain_dir = '/mnt/nas2/data/systematicReview/PICO_datasets/annotation_me/PICO_test_ebm_pis/plain.html/pool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146edca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_files(mypath):\n",
    "    list_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    dict_files = x = { i.split('-')[-1].split('.')[0] : i for i in list_files }\n",
    "    return dict_files\n",
    "\n",
    "plain_files = dir_files(plain_dir)\n",
    "annot_files = dir_files(annot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a4c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_char_annot(plain_dir, plain_files, annot_dir, annot_files):\n",
    "    annotations = dict()\n",
    "    plain_text_dict = dict()\n",
    "\n",
    "    for k,v in plain_files.items():\n",
    "\n",
    "        plain_file_path = plain_dir + '/' + v\n",
    "        annot_file_path = annot_dir + '/' + annot_files[k]\n",
    "\n",
    "        with open( annot_file_path, 'r' ) as af, open( plain_file_path, 'r' ) as pf:\n",
    "            annot_json = json.loads(af.read())\n",
    "\n",
    "            plainfile_soup = BeautifulSoup(pf)\n",
    "            text_list = plainfile_soup.find_all(\"pre\")\n",
    "\n",
    "            annotation_collect = []\n",
    "            annotation_dict = dict()\n",
    "            document_parts = {}\n",
    "\n",
    "\n",
    "            for e in annot_json['entities']:\n",
    "                document_part = e['part']\n",
    "                text_annot = e['offsets'][0]['text']\n",
    "                annot_start = e['offsets'][0]['start']\n",
    "                annot_end = annot_start + len(e['offsets'][0]['text'])            \n",
    "                \n",
    "                \n",
    "                if len(text_list) == 1:\n",
    "                    plain_text = text_list[0].text\n",
    "                    document_parts[ text_list[0].get('id') ] = text_list[0].text\n",
    "                    document_entity_match = plain_text[annot_start:annot_end]\n",
    "                    document_entity_match_label = e['classId']\n",
    "\n",
    "                    assert document_entity_match.strip() == text_annot.strip()\n",
    "\n",
    "                    # Character level annotation\n",
    "                    match_label_list = len( text_annot.strip() ) * [document_entity_match_label]\n",
    "                    document_char_labels = [0] * len(plain_text)\n",
    "                    document_char_labels[annot_start:annot_end] = match_label_list\n",
    "\n",
    "                    if document_part not in annotation_dict:\n",
    "                        annotation_dict[document_part] = [ document_char_labels ]\n",
    "                    else:\n",
    "                        annotation_dict[document_part].append( document_char_labels )\n",
    "            \n",
    "            plain_text_dict[k] = document_parts\n",
    "            annotations[k] = annotation_dict\n",
    "            \n",
    "    return annotations, plain_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc6b6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_annot, plain_texts = fetch_char_annot(plain_dir, plain_files, annot_dir, annot_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "371b381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent2binlabs(lst, picos):\n",
    "    \n",
    "    if picos == 'p':\n",
    "        ignore_labs = list(S_ents.values()) + list(I_ents.values())\n",
    "    if picos == 'i':\n",
    "        ignore_labs = list(S_ents.values()) + list(P_ents.values())\n",
    "    if picos == 'o':\n",
    "        ignore_labs = list(S_ents.values())\n",
    "    if picos == 's':\n",
    "        ignore_labs = list(P_ents.values()) + list(I_ents.values())\n",
    "           \n",
    "    new_lst = []\n",
    "    \n",
    "    for l in lst:\n",
    "        if len(l) == 1 and ( l[0] == 0 or ent2name[ l[0] ] not in ignore_labs):\n",
    "            new_lst.append( ['0'] )\n",
    "            \n",
    "        elif len(l) == 1:\n",
    "            print( ent2name[ l[0] ] )\n",
    "            new_lst.append( ['1'] )            \n",
    "    \n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d9ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergelabels(old_labels, labels_clean):\n",
    "    \n",
    "    #old_labels = ann[k_a][k_a_]['p'] # Fetch already stored labels\n",
    "    #print( old_labels )\n",
    "    new_labels = [ list(set(old_labels[n] + l)) for n, l in enumerate(labels_clean)] # Append new labels to the old ones\n",
    "    # convert new labels (which is a list of lists to flattened list)\n",
    "    new_labels = [ ['1'] if len(nl) > 1 else ['0'] for nl in new_labels ]\n",
    "\n",
    "    assert len( old_labels ) == len( new_labels )\n",
    "    \n",
    "    return new_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e05db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2tokAnnot(char_annotations, plain_text_dict):\n",
    "    \n",
    "    token_annotations = {}\n",
    "    \n",
    "    for k_a, v_a in char_annotations.items():\n",
    "        #print(k_a) # document number\n",
    "        \n",
    "        token_annotations[k_a] = {}\n",
    "\n",
    "        for k_a_, v_a_ in v_a.items(): # convert char annot to tok annot\n",
    "            #print(k_a_) # document part\n",
    "\n",
    "            # Get text\n",
    "            text =  plain_text_dict[k_a][k_a_]\n",
    "            \n",
    "            # Get POS tags\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            text_pos = [token.tag_ for token in doc]\n",
    "            text_tokens = [token.text for token in doc]\n",
    "            text_abs_offset = [( token.idx, token.idx+len(token.text) ) for token in doc]\n",
    "            \n",
    "            # Iterate through the annotations for each text part\n",
    "            \n",
    "            part_labels = []\n",
    "            \n",
    "            for v_a_i in v_a_:\n",
    "            \n",
    "                labels = [ list( set(v_a_i[ ws[0] : ws[1] ]) ) for ws in  text_abs_offset ]\n",
    "                # if the length of label is more than 1, then only keep the non-zero labels\n",
    "                labels_clean = [ list(filter(lambda num: num != 0, l)) if len( l ) > 1 else l for l in labels ]\n",
    "                \n",
    "                assert len(labels) == len(labels_clean)\n",
    "                \n",
    "                labels_ent = [ [ ent2name[l[0]] ] if l[0] in ent2name else [ l[0] ] for l in labels_clean ]\n",
    "                assert len(labels) == len(labels_clean) == len(labels_ent)\n",
    "                \n",
    "                if len(part_labels) == 0:\n",
    "                    part_labels = labels_ent\n",
    "                    #print( part_labels )\n",
    "                else:\n",
    "                    # merge labels_ent with part_labels\n",
    "                    for counter, l in enumerate(labels_ent):\n",
    "                        part_labels[ counter ].append( l[0] )\n",
    "            \n",
    "            # Remove redundant labels after merging\n",
    "            part_labels = [list(set(l)) for l in part_labels]\n",
    "            \n",
    "            # bifurcate the P, I, O labels\n",
    "            part_labels_p = []\n",
    "            part_labels_i = []\n",
    "            part_labels_s = []\n",
    "            for l in part_labels:\n",
    "                if len(l) == 1:\n",
    "                    part_labels_p.append( l[0] )\n",
    "                    part_labels_i.append( l[0] )\n",
    "                    part_labels_s.append( l[0] )\n",
    "                else:\n",
    "                    p_reg = re.compile('Participant_.*')\n",
    "                    i_reg = re.compile('Intervention_.*')\n",
    "                    s_reg = re.compile('StudyType')\n",
    "\n",
    "                    if any(p_reg.match(str(line)) for line in l) == True :\n",
    "                        part_labels_p.append( str(1) )\n",
    "                    else:\n",
    "                        part_labels_p.append( str(0) )\n",
    "\n",
    "                    if any(i_reg.match(str(line)) for line in l) == True :\n",
    "                        part_labels_i.append( str(1) )\n",
    "                    else:\n",
    "                        part_labels_i.append( str(0) )\n",
    "\n",
    "                    if any(s_reg.match(str(line)) for line in l) == True :\n",
    "                        part_labels_s.append( str(1) )\n",
    "                    else:\n",
    "                        part_labels_s.append( str(0) )\n",
    "\n",
    "\n",
    "            token_annotations[k_a][k_a_] = {}\n",
    "            token_annotations[k_a][k_a_]['text'] = ' '.join(text_tokens)\n",
    "            token_annotations[k_a][k_a_]['tokens'] = text_tokens\n",
    "            token_annotations[k_a][k_a_]['labels'] = part_labels\n",
    "            token_annotations[k_a][k_a_]['pos'] = text_pos\n",
    "            token_annotations[k_a][k_a_]['abs_char_offsets'] = [token.idx for token in doc]\n",
    "            \n",
    "            token_annotations[k_a][k_a_]['participant_fine'] = part_labels_p\n",
    "            token_annotations[k_a][k_a_]['intervention_fine'] = part_labels_i\n",
    "            token_annotations[k_a][k_a_]['studytype_fine'] = part_labels_s\n",
    "            \n",
    "            token_annotations[k_a][k_a_]['participant'] = part_labels_p\n",
    "            token_annotations[k_a][k_a_]['intervention'] = part_labels_i\n",
    "            token_annotations[k_a][k_a_]['studytype'] = part_labels_s\n",
    "\n",
    "\n",
    "    return token_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81931551",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_annotations = char2tokAnnot(char_annot, plain_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0b7332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s1v1': {'text': 'Sensory Adapted Dental Environments to Enhance Oral Care for Children with Autism Spectrum Disorders : A Randomized Controlled Pilot Study . \\n\\n This pilot and feasibility study examined the impact of a sensory adapted dental environment ( SADE ) to reduce distress , sensory discomfort , and perception of pain during oral prophylaxis for children with autism spectrum disorder ( ASD ) . Participants were 44 children ages 6 - 12 ( n = 22 typical , n = 22 ASD ) . In an experimental crossover design , each participant underwent two professional dental cleanings , one in a regular dental environment ( RDE ) and one in a SADE , administered in a randomized and counterbalanced order 3 - 4 months apart . Outcomes included measures of physiological anxiety , behavioral distress , pain intensity , and sensory discomfort . Both groups exhibited decreased physiological anxiety and reported lower pain and sensory discomfort in the SADE condition compared to RDE , indicating a beneficial effect of the SADE . \\n\\n',\n",
       "  'tokens': ['Sensory',\n",
       "   'Adapted',\n",
       "   'Dental',\n",
       "   'Environments',\n",
       "   'to',\n",
       "   'Enhance',\n",
       "   'Oral',\n",
       "   'Care',\n",
       "   'for',\n",
       "   'Children',\n",
       "   'with',\n",
       "   'Autism',\n",
       "   'Spectrum',\n",
       "   'Disorders',\n",
       "   ':',\n",
       "   'A',\n",
       "   'Randomized',\n",
       "   'Controlled',\n",
       "   'Pilot',\n",
       "   'Study',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'This',\n",
       "   'pilot',\n",
       "   'and',\n",
       "   'feasibility',\n",
       "   'study',\n",
       "   'examined',\n",
       "   'the',\n",
       "   'impact',\n",
       "   'of',\n",
       "   'a',\n",
       "   'sensory',\n",
       "   'adapted',\n",
       "   'dental',\n",
       "   'environment',\n",
       "   '(',\n",
       "   'SADE',\n",
       "   ')',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'distress',\n",
       "   ',',\n",
       "   'sensory',\n",
       "   'discomfort',\n",
       "   ',',\n",
       "   'and',\n",
       "   'perception',\n",
       "   'of',\n",
       "   'pain',\n",
       "   'during',\n",
       "   'oral',\n",
       "   'prophylaxis',\n",
       "   'for',\n",
       "   'children',\n",
       "   'with',\n",
       "   'autism',\n",
       "   'spectrum',\n",
       "   'disorder',\n",
       "   '(',\n",
       "   'ASD',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Participants',\n",
       "   'were',\n",
       "   '44',\n",
       "   'children',\n",
       "   'ages',\n",
       "   '6',\n",
       "   '-',\n",
       "   '12',\n",
       "   '(',\n",
       "   'n',\n",
       "   '=',\n",
       "   '22',\n",
       "   'typical',\n",
       "   ',',\n",
       "   'n',\n",
       "   '=',\n",
       "   '22',\n",
       "   'ASD',\n",
       "   ')',\n",
       "   '.',\n",
       "   'In',\n",
       "   'an',\n",
       "   'experimental',\n",
       "   'crossover',\n",
       "   'design',\n",
       "   ',',\n",
       "   'each',\n",
       "   'participant',\n",
       "   'underwent',\n",
       "   'two',\n",
       "   'professional',\n",
       "   'dental',\n",
       "   'cleanings',\n",
       "   ',',\n",
       "   'one',\n",
       "   'in',\n",
       "   'a',\n",
       "   'regular',\n",
       "   'dental',\n",
       "   'environment',\n",
       "   '(',\n",
       "   'RDE',\n",
       "   ')',\n",
       "   'and',\n",
       "   'one',\n",
       "   'in',\n",
       "   'a',\n",
       "   'SADE',\n",
       "   ',',\n",
       "   'administered',\n",
       "   'in',\n",
       "   'a',\n",
       "   'randomized',\n",
       "   'and',\n",
       "   'counterbalanced',\n",
       "   'order',\n",
       "   '3',\n",
       "   '-',\n",
       "   '4',\n",
       "   'months',\n",
       "   'apart',\n",
       "   '.',\n",
       "   'Outcomes',\n",
       "   'included',\n",
       "   'measures',\n",
       "   'of',\n",
       "   'physiological',\n",
       "   'anxiety',\n",
       "   ',',\n",
       "   'behavioral',\n",
       "   'distress',\n",
       "   ',',\n",
       "   'pain',\n",
       "   'intensity',\n",
       "   ',',\n",
       "   'and',\n",
       "   'sensory',\n",
       "   'discomfort',\n",
       "   '.',\n",
       "   'Both',\n",
       "   'groups',\n",
       "   'exhibited',\n",
       "   'decreased',\n",
       "   'physiological',\n",
       "   'anxiety',\n",
       "   'and',\n",
       "   'reported',\n",
       "   'lower',\n",
       "   'pain',\n",
       "   'and',\n",
       "   'sensory',\n",
       "   'discomfort',\n",
       "   'in',\n",
       "   'the',\n",
       "   'SADE',\n",
       "   'condition',\n",
       "   'compared',\n",
       "   'to',\n",
       "   'RDE',\n",
       "   ',',\n",
       "   'indicating',\n",
       "   'a',\n",
       "   'beneficial',\n",
       "   'effect',\n",
       "   'of',\n",
       "   'the',\n",
       "   'SADE',\n",
       "   '.',\n",
       "   '\\n\\n'],\n",
       "  'labels': [['Intervention_name', 0],\n",
       "   ['Intervention_name', 0],\n",
       "   ['Intervention_name', 0],\n",
       "   ['Intervention_name', 0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Participant_age'],\n",
       "   [0],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'StudyType'],\n",
       "   [0, 'StudyType'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Participant_age'],\n",
       "   [0],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0, 'Participant_samplesize', 'Participant_age'],\n",
       "   [0, 'Participant_age'],\n",
       "   [0, 'Participant_age'],\n",
       "   [0, 'Participant_age'],\n",
       "   [0, 'Participant_age'],\n",
       "   [0],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0, 'Participant_samplesize'],\n",
       "   [0, 'Participant_disease'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'StudyType'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0, 'Intervention_name'],\n",
       "   [0],\n",
       "   [0]],\n",
       "  'pos': ['NNP',\n",
       "   'NNP',\n",
       "   'NNP',\n",
       "   'NNS',\n",
       "   'IN',\n",
       "   'VB',\n",
       "   'NNP',\n",
       "   'NNP',\n",
       "   'IN',\n",
       "   'NNP',\n",
       "   'IN',\n",
       "   'NNP',\n",
       "   'NNP',\n",
       "   'NNS',\n",
       "   ':',\n",
       "   'DT',\n",
       "   'NNP',\n",
       "   'NNP',\n",
       "   'NNP',\n",
       "   'NNP',\n",
       "   '.',\n",
       "   '_SP',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   'CC',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'VBD',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'JJ',\n",
       "   'VBN',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   '-LRB-',\n",
       "   'NN',\n",
       "   '-RRB-',\n",
       "   'TO',\n",
       "   'VB',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'CC',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'NNS',\n",
       "   'IN',\n",
       "   'NN',\n",
       "   'VBP',\n",
       "   'NN',\n",
       "   '-LRB-',\n",
       "   'NNP',\n",
       "   '-RRB-',\n",
       "   '.',\n",
       "   'NNS',\n",
       "   'VBD',\n",
       "   'CD',\n",
       "   'NNS',\n",
       "   'NNS',\n",
       "   'CD',\n",
       "   'SYM',\n",
       "   'CD',\n",
       "   '-LRB-',\n",
       "   'CC',\n",
       "   'SYM',\n",
       "   'CD',\n",
       "   'JJ',\n",
       "   ',',\n",
       "   'CC',\n",
       "   'SYM',\n",
       "   'CD',\n",
       "   'NNP',\n",
       "   '-RRB-',\n",
       "   '.',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   'VBD',\n",
       "   'CD',\n",
       "   'JJ',\n",
       "   'JJ',\n",
       "   'NNS',\n",
       "   ',',\n",
       "   'CD',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'JJ',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   '-LRB-',\n",
       "   'NNP',\n",
       "   '-RRB-',\n",
       "   'CC',\n",
       "   'CD',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'VBN',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'JJ',\n",
       "   'CC',\n",
       "   'VBN',\n",
       "   'NN',\n",
       "   'CD',\n",
       "   'SYM',\n",
       "   'CD',\n",
       "   'NNS',\n",
       "   'RB',\n",
       "   '.',\n",
       "   'NNS',\n",
       "   'VBD',\n",
       "   'NNS',\n",
       "   'IN',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   ',',\n",
       "   'CC',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   '.',\n",
       "   'DT',\n",
       "   'NNS',\n",
       "   'VBD',\n",
       "   'VBD',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'CC',\n",
       "   'VBD',\n",
       "   'JJR',\n",
       "   'NN',\n",
       "   'CC',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'VBN',\n",
       "   'IN',\n",
       "   'NNP',\n",
       "   ',',\n",
       "   'VBG',\n",
       "   'DT',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   '.',\n",
       "   '_SP'],\n",
       "  'abs_char_offsets': [0,\n",
       "   8,\n",
       "   16,\n",
       "   23,\n",
       "   36,\n",
       "   39,\n",
       "   47,\n",
       "   52,\n",
       "   57,\n",
       "   61,\n",
       "   70,\n",
       "   75,\n",
       "   82,\n",
       "   91,\n",
       "   100,\n",
       "   102,\n",
       "   104,\n",
       "   115,\n",
       "   126,\n",
       "   132,\n",
       "   137,\n",
       "   138,\n",
       "   140,\n",
       "   145,\n",
       "   151,\n",
       "   155,\n",
       "   167,\n",
       "   173,\n",
       "   182,\n",
       "   186,\n",
       "   193,\n",
       "   196,\n",
       "   198,\n",
       "   206,\n",
       "   214,\n",
       "   221,\n",
       "   233,\n",
       "   234,\n",
       "   238,\n",
       "   240,\n",
       "   243,\n",
       "   250,\n",
       "   258,\n",
       "   260,\n",
       "   268,\n",
       "   278,\n",
       "   280,\n",
       "   284,\n",
       "   295,\n",
       "   298,\n",
       "   303,\n",
       "   310,\n",
       "   315,\n",
       "   327,\n",
       "   331,\n",
       "   340,\n",
       "   345,\n",
       "   352,\n",
       "   361,\n",
       "   370,\n",
       "   371,\n",
       "   374,\n",
       "   375,\n",
       "   377,\n",
       "   390,\n",
       "   395,\n",
       "   398,\n",
       "   407,\n",
       "   412,\n",
       "   413,\n",
       "   414,\n",
       "   417,\n",
       "   418,\n",
       "   420,\n",
       "   422,\n",
       "   425,\n",
       "   432,\n",
       "   434,\n",
       "   436,\n",
       "   438,\n",
       "   441,\n",
       "   444,\n",
       "   445,\n",
       "   447,\n",
       "   450,\n",
       "   453,\n",
       "   466,\n",
       "   476,\n",
       "   482,\n",
       "   484,\n",
       "   489,\n",
       "   501,\n",
       "   511,\n",
       "   515,\n",
       "   528,\n",
       "   535,\n",
       "   544,\n",
       "   546,\n",
       "   550,\n",
       "   553,\n",
       "   555,\n",
       "   563,\n",
       "   570,\n",
       "   582,\n",
       "   583,\n",
       "   586,\n",
       "   588,\n",
       "   592,\n",
       "   596,\n",
       "   599,\n",
       "   601,\n",
       "   605,\n",
       "   607,\n",
       "   620,\n",
       "   623,\n",
       "   625,\n",
       "   636,\n",
       "   640,\n",
       "   656,\n",
       "   662,\n",
       "   663,\n",
       "   664,\n",
       "   666,\n",
       "   673,\n",
       "   678,\n",
       "   680,\n",
       "   689,\n",
       "   698,\n",
       "   707,\n",
       "   710,\n",
       "   724,\n",
       "   731,\n",
       "   733,\n",
       "   744,\n",
       "   752,\n",
       "   754,\n",
       "   759,\n",
       "   768,\n",
       "   770,\n",
       "   774,\n",
       "   782,\n",
       "   792,\n",
       "   794,\n",
       "   799,\n",
       "   806,\n",
       "   816,\n",
       "   826,\n",
       "   840,\n",
       "   848,\n",
       "   852,\n",
       "   861,\n",
       "   867,\n",
       "   872,\n",
       "   876,\n",
       "   884,\n",
       "   895,\n",
       "   898,\n",
       "   902,\n",
       "   907,\n",
       "   917,\n",
       "   926,\n",
       "   929,\n",
       "   932,\n",
       "   934,\n",
       "   945,\n",
       "   947,\n",
       "   958,\n",
       "   965,\n",
       "   968,\n",
       "   972,\n",
       "   976,\n",
       "   977],\n",
       "  'participant_fine': ['0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0],\n",
       "  'intervention_fine': ['1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0],\n",
       "  'participant': ['0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   0],\n",
       "  'intervention': ['1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   '1',\n",
       "   0,\n",
       "   0]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_annotations['25931290']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f7f7a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18773733\n",
      "17293018\n",
      "10940525\n",
      "6920252\n",
      "7814711\n",
      "21170734\n",
      "21393467\n",
      "24660757\n"
     ]
    }
   ],
   "source": [
    "# get rid of the document part\n",
    "annotations_final = dict()\n",
    "\n",
    "for k_doc, v_doc in tok_annotations.items():\n",
    "    \n",
    "    #if k_doc not in annotations_final:\n",
    "    #    annotations_final[k_doc] = {}\n",
    "        \n",
    "    # document part: 's1v1'\n",
    "    if 's1v1' in v_doc:\n",
    "        annotations_final[k_doc] = {}\n",
    "        annotations_final[k_doc]['text'] = v_doc['s1v1']['text']\n",
    "        annotations_final[k_doc]['tokens'] = v_doc['s1v1']['tokens']\n",
    "        annotations_final[k_doc]['pos'] = v_doc['s1v1']['pos']\n",
    "        annotations_final[k_doc]['abs_char_offsets'] = v_doc['s1v1']['abs_char_offsets']        \n",
    "        annotations_final[k_doc]['participants_fine'] = v_doc['s1v1']['participant_fine']\n",
    "        annotations_final[k_doc]['participants'] = v_doc['s1v1']['participant']\n",
    "        annotations_final[k_doc]['interventions_fine'] = v_doc['s1v1']['intervention_fine']\n",
    "        annotations_final[k_doc]['interventions'] = v_doc['s1v1']['intervention']\n",
    "    else:\n",
    "        print(k_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0034e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bifurcate the P and I annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7469f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6f07f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/nas2/data/systematicReview/PICO_datasets/EBM_parsed/test_ebm_anjani.json', 'w+') as wf:\n",
    "    json.dump(annotations_final, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5f773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21aa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2792122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
